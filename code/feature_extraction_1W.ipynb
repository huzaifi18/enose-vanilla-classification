{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaefdfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "pd.set_option('display.max_columns', 41)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43f97d2",
   "metadata": {},
   "source": [
    "Data yang digunakan adalah data yang telah dikoreksi baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57a35fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_csv_file = glob.glob(\"../data_baseline/_A*.csv\") # Mencari semua berkas CSV yang dimulai dengan 'A' dalam direktori 'data'\n",
    "B_csv_file = glob.glob(\"../data_baseline/_B*.csv\") # Mencari semua berkas CSV yang dimulai dengan 'B' dalam direktori 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4f45d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MaxGradient(dataframe, column):\n",
    "    array = dataframe[column].to_numpy()\n",
    "    return max(np.gradient(array, edge_order=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba41ffe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_FeaturesCubic(CSV_files, label, return_dataset=False):\n",
    "    \"\"\"extract max, mean, median, and gradients from given CSV files\n",
    "\n",
    "    Args:\n",
    "        CSV_files (list): csv files\n",
    "        label (str): label\n",
    "        return_dataset (bool, optional): wheter get formated dataset. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: if return_dataset is True\n",
    "        array: if return_dataset is False\n",
    "    \"\"\"\n",
    "    maxi = np.empty((0, 10))\n",
    "    mean = np.empty((0, 10))\n",
    "    median = np.empty((0, 10))\n",
    "    grad = np.empty((0, 10))\n",
    "    for csv_file in CSV_files:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        df.drop(columns=[\"time(s)\",\"Temp\",\"Humid\"], inplace=True)\n",
    "        _max_list = []\n",
    "        _mean_list = []\n",
    "        _median_list = []\n",
    "        _grad_list = []\n",
    "        for cols, _ in df.iteritems():        \n",
    "            _max_list.append(df[cols].max())\n",
    "            _mean_list.append(df[cols].mean())\n",
    "            _median_list.append(df[cols].median())\n",
    "            _grad_list.append(get_MaxGradient(df, cols))\n",
    "        maxi = np.vstack((maxi, np.array(_max_list).T))\n",
    "        mean = np.vstack((mean, np.array(_mean_list).T))\n",
    "        median = np.vstack((median, np.array(_median_list).T))\n",
    "        grad = np.vstack((grad, np.array(_grad_list).T))\n",
    "    \n",
    "    if return_dataset:\n",
    "        mean_df = pd.DataFrame(mean, columns=[\"mu1\", \"mu2\", \"mu3\", \"mu4\", \"mu5\", \"mu6\", \"mu7\", \"mu8\", \"mu9\", \"mu10\"])\n",
    "        max_df = pd.DataFrame(maxi, columns=[\"max1\", \"max2\", \"max3\", \"max4\", \"max5\", \"max6\", \"mu7\", \"max8\", \"max9\", \"max10\"])\n",
    "        median_df = pd.DataFrame(median, columns=[\"med1\", \"med2\", \"med3\", \"med4\", \"med5\", \"med6\", \"med7\", \"med8\", \"med9\", \"med10\"])\n",
    "        grad_df = pd.DataFrame(grad, columns=[\"grad1\", \"grad2\", \"grad3\", \"grad4\", \"grad5\", \"grad6\", \"grad7\", \"grad8\", \"grad9\", \"grad10\"])\n",
    "        dataset = pd.concat([mean_df, max_df, median_df, grad_df], axis=1)\n",
    "        dataset[\"label\"] = label\n",
    "        return dataset\n",
    "        \n",
    "    else:        \n",
    "        all_arrays = [maxi, mean, median, grad]\n",
    "        all_arrays = np.transpose(all_arrays, (1, 2, 0))\n",
    "        return all_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67ec1479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35, 10, 4)\n",
      "(35, 10, 4)\n"
     ]
    }
   ],
   "source": [
    "A_array = extract_FeaturesCubic(A_csv_file, label=\"A\")\n",
    "B_array = extract_FeaturesCubic(B_csv_file, label=\"B\")\n",
    "print(A_array.shape)\n",
    "print(B_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76ddf544",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_array_file_filename = 'A_array_1W.npy'\n",
    "B_array_file_filename = 'B_array_1W.npy'\n",
    "\n",
    "np.save(f\"../Exp/{A_array_file_filename}\", A_array)\n",
    "np.save(f\"../Exp/{B_array_file_filename}\", B_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bbbc09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35, 10, 4)\n",
      "(35, 10, 4)\n"
     ]
    }
   ],
   "source": [
    "A_tensor =  tf.convert_to_tensor(A_array)\n",
    "B_tensor =  tf.convert_to_tensor(B_array)\n",
    "print(A_tensor.shape)\n",
    "print(B_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31c13b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_tensor_file_filename = 'A_tensor_1W.tf'\n",
    "B_tensor_file_filename = 'B_tensor_1W.tf'\n",
    "\n",
    "tf.io.write_file(f\"../Exp/{A_tensor_file_filename}\", tf.io.serialize_tensor(A_tensor))\n",
    "tf.io.write_file(f\"../Exp/{B_tensor_file_filename}\", tf.io.serialize_tensor(B_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04509b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to load\n",
    "# tensor_bytes = tf.io.read_file(file_path)\n",
    "\n",
    "# tensor_tf = tf.io.parse_tensor(tensor_bytes, out_type=tf.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee1c426",
   "metadata": {},
   "source": [
    "###  Get Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0ff58d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_dataset = extract_FeaturesCubic(A_csv_file, label=\"A\", return_dataset=True)\n",
    "B_dataset = extract_FeaturesCubic(B_csv_file, label=\"B\", return_dataset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea0fa172",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = pd.concat([A_dataset, B_dataset], axis=0)\n",
    "full_dataset.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90fcd5b",
   "metadata": {},
   "source": [
    "#### Save to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01de07c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset.to_csv(\"../dataset/data_2c40d1w.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
